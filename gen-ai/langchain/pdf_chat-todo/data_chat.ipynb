{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: vector_db/chroma/\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Embedding' object has no attribute 'embed_documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[1;32m      4\u001b[0m persist_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvector_db/chroma/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m vectordb \u001b[38;5;241m=\u001b[39m Chroma(persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory, embedding_function\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdistilbert\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mword_embeddings)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(vectordb\u001b[38;5;241m.\u001b[39m_collection\u001b[38;5;241m.\u001b[39mcount())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llm/lib/python3.12/site-packages/langchain/vectorstores/chroma.py:95\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;241m=\u001b[39m embedding_function\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persist_directory \u001b[38;5;241m=\u001b[39m persist_directory\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mget_or_create_collection(\n\u001b[1;32m     94\u001b[0m     name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[0;32m---> 95\u001b[0m     embedding_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function\u001b[38;5;241m.\u001b[39membed_documents\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     98\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mcollection_metadata,\n\u001b[1;32m     99\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llm/lib/python3.12/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Embedding' object has no attribute 'embed_documents'"
     ]
    }
   ],
   "source": [
    "# Load vector database that was persisted earlier and check collection count in it\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "persist_directory = 'vector_db/chroma/'\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoIndexException",
     "evalue": "Index not found, please create an instance before querying",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoIndexException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is Quasarion?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m docs \u001b[38;5;241m=\u001b[39m vectordb\u001b[38;5;241m.\u001b[39msimilarity_search(question,k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mlen\u001b[39m(docs)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m docs:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llm/lib/python3.12/site-packages/langchain/vectorstores/chroma.py:171\u001b[0m, in \u001b[0;36mChroma.similarity_search\u001b[0;34m(self, query, k, filter, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search\u001b[39m(\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    156\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    160\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run similarity search with Chroma.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03m        List[Document]: List of documents most similar to the query text.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m     docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_search_with_score(query, k, \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m)\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llm/lib/python3.12/site-packages/langchain/vectorstores/chroma.py:219\u001b[0m, in \u001b[0;36mChroma.similarity_search_with_score\u001b[0;34m(self, query, k, filter, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function\u001b[38;5;241m.\u001b[39membed_query(query)\n\u001b[0;32m--> 219\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__query_collection(\n\u001b[1;32m    220\u001b[0m         query_embeddings\u001b[38;5;241m=\u001b[39m[query_embedding], n_results\u001b[38;5;241m=\u001b[39mk, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m\n\u001b[1;32m    221\u001b[0m     )\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _results_to_docs_and_scores(results)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llm/lib/python3.12/site-packages/langchain/utils.py:53\u001b[0m, in \u001b[0;36mxor_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     invalid_group_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(arg_groups[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m invalid_groups]\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExactly one argument in each of the following\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m groups must be defined:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(invalid_group_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m     )\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llm/lib/python3.12/site-packages/langchain/vectorstores/chroma.py:118\u001b[0m, in \u001b[0;36mChroma.__query_collection\u001b[0;34m(self, query_texts, query_embeddings, n_results, where, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import chromadb python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install chromadb`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m     )\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection\u001b[38;5;241m.\u001b[39mquery(\n\u001b[1;32m    119\u001b[0m     query_texts\u001b[38;5;241m=\u001b[39mquery_texts,\n\u001b[1;32m    120\u001b[0m     query_embeddings\u001b[38;5;241m=\u001b[39mquery_embeddings,\n\u001b[1;32m    121\u001b[0m     n_results\u001b[38;5;241m=\u001b[39mn_results,\n\u001b[1;32m    122\u001b[0m     where\u001b[38;5;241m=\u001b[39mwhere,\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    124\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llm/lib/python3.12/site-packages/chromadb/api/models/Collection.py:227\u001b[0m, in \u001b[0;36mCollection.query\u001b[0;34m(self, query_embeddings, query_texts, n_results, where, where_document, include)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m where_document \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     where_document \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_query(\n\u001b[1;32m    228\u001b[0m     collection_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m    229\u001b[0m     query_embeddings\u001b[38;5;241m=\u001b[39mquery_embeddings,\n\u001b[1;32m    230\u001b[0m     n_results\u001b[38;5;241m=\u001b[39mn_results,\n\u001b[1;32m    231\u001b[0m     where\u001b[38;5;241m=\u001b[39mwhere,\n\u001b[1;32m    232\u001b[0m     where_document\u001b[38;5;241m=\u001b[39mwhere_document,\n\u001b[1;32m    233\u001b[0m     include\u001b[38;5;241m=\u001b[39minclude,\n\u001b[1;32m    234\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llm/lib/python3.12/site-packages/chromadb/api/local.py:437\u001b[0m, in \u001b[0;36mLocalAPI._query\u001b[0;34m(self, collection_id, query_embeddings, n_results, where, where_document, include)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_query\u001b[39m(\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    430\u001b[0m     collection_id: UUID,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    435\u001b[0m     include: Include \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadatas\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistances\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    436\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m QueryResult:\n\u001b[0;32m--> 437\u001b[0m     uuids, distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_db\u001b[38;5;241m.\u001b[39mget_nearest_neighbors(\n\u001b[1;32m    438\u001b[0m         collection_uuid\u001b[38;5;241m=\u001b[39mcollection_id,\n\u001b[1;32m    439\u001b[0m         where\u001b[38;5;241m=\u001b[39mwhere,\n\u001b[1;32m    440\u001b[0m         where_document\u001b[38;5;241m=\u001b[39mwhere_document,\n\u001b[1;32m    441\u001b[0m         embeddings\u001b[38;5;241m=\u001b[39mquery_embeddings,\n\u001b[1;32m    442\u001b[0m         n_results\u001b[38;5;241m=\u001b[39mn_results,\n\u001b[1;32m    443\u001b[0m     )\n\u001b[1;32m    445\u001b[0m     include_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m include\n\u001b[1;32m    446\u001b[0m     include_documents \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m include\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llm/lib/python3.12/site-packages/chromadb/db/clickhouse.py:585\u001b[0m, in \u001b[0;36mClickhouse.get_nearest_neighbors\u001b[0;34m(self, collection_uuid, where, where_document, embeddings, n_results)\u001b[0m\n\u001b[1;32m    582\u001b[0m     ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    584\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index(collection_uuid)\n\u001b[0;32m--> 585\u001b[0m uuids, distances \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mget_nearest_neighbors(embeddings, n_results, ids)\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m uuids, distances\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llm/lib/python3.12/site-packages/chromadb/db/index/hnswlib.py:240\u001b[0m, in \u001b[0;36mHnswlib.get_nearest_neighbors\u001b[0;34m(self, query, k, ids)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_nearest_neighbors\u001b[39m(\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28mself\u001b[39m, query: Embeddings, k: \u001b[38;5;28mint\u001b[39m, ids: Optional[List[UUID]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[List[UUID]], List[List[\u001b[38;5;28mfloat\u001b[39m]]]:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NoIndexException(\n\u001b[1;32m    241\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex not found, please create an instance before querying\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m         )\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# Check dimensionality\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_dimensionality(query)\n",
      "\u001b[0;31mNoIndexException\u001b[0m: Index not found, please create an instance before querying"
     ]
    }
   ],
   "source": [
    "question = \"what is Quasarion?\"\n",
    "docs = vectordb.similarity_search(question,k=3)\n",
    "len(docs)\n",
    "for res in docs:\n",
    "    print (res.page_content)\n",
    "    print ('============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(28996, 768, padding_idx=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DistilBertForQuestionAnswering, pipeline\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "model = AutoModelForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "\n",
    "# Load model directly\n",
    "\n",
    "pipe = pipeline(\"question-answering\", \n",
    "                model=model, \n",
    "                tokenizer=tokenizer,     \n",
    "                temperature=0.2,\n",
    "                max_new_tokens=512)\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline = pipe,\n",
    "    model_kwargs={\"temperature\": 0.2, \"max_length\": 1024},\n",
    ")\n",
    "model.distilbert.embeddings.word_embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(28996, 768, padding_idx=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.distilbert.embeddings.word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectordb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is Quasarion ?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m matching_docs \u001b[38;5;241m=\u001b[39m vectordb\u001b[38;5;241m.\u001b[39mmax_marginal_relevance_search(query)\n\u001b[1;32m      7\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m matching_docs:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectordb' is not defined"
     ]
    }
   ],
   "source": [
    "# from langchain.chains.question_answering import load_qa_chain\n",
    "# chain = load_qa_chain(llm, chain_type=\"stuff\",verbose=True)\n",
    "import torch\n",
    "query = \"What is Quasarion ?\"\n",
    "matching_docs = vectordb.max_marginal_relevance_search(query)\n",
    "\n",
    "context = ''\n",
    "for i in matching_docs:\n",
    "    context += \" \" + i.page_content\n",
    "\n",
    "inputs = tokenizer(question, context[:512], return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()\n",
    "if answer_start_index > answer_end_index:\n",
    "    answer_end_index, answer_start_index = answer_start_index, answer_end_index\n",
    "print (answer_start_index, answer_end_index)\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "print (predict_answer_tokens)\n",
    "tokenizer.decode(predict_answer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.2069, -0.0259,  0.3412,  0.0153,  0.4669,  0.2928,  0.0926,  0.0060,\n",
       "           0.4558,  0.2290,  0.2784,  0.2105,  0.1995,  0.0926]],\n",
       "        grad_fn=<CloneBackward0>),\n",
       " tensor([[-0.3415,  0.0370, -0.1975, -0.4639, -0.0913,  0.3339,  0.2071, -0.4206,\n",
       "          -0.1138, -0.1672, -0.3959, -0.2285, -0.1096,  0.2071]],\n",
       "        grad_fn=<CloneBackward0>))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "\n",
    "inputs = tokenizer(question, text, return_tensors='pt')\n",
    "\n",
    "start_positions = torch.tensor([1])\n",
    "end_positions = torch.tensor([3])\n",
    "\n",
    "outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)\n",
    "loss = outputs.loss\n",
    "\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits \n",
    "start_scores, end_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QCML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
