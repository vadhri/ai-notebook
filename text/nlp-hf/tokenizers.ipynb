{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizers split the text fed into training as input ids and attention masks.\n",
    "The style of tokenization depends on the style of the tokenizer, there could be tokenizations done at diferent levels \n",
    "- bytes\n",
    "- characters\n",
    "- subwords/sequences\n",
    "- words\n",
    "- sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate tokens in HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids tensor([[  101,  1188,  1850, 22615,  2993,  1106,  1129,  3088,  1154,  3423,\n",
      "           119,   102]])\n",
      "token_type_ids tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] This sentense needs to be broken into pieces. [SEP]'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "raw_inputs = [\n",
    "    \"This sentense needs to be broken into pieces.\"\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "for k,v in inputs.items():\n",
    "    print (k,v)\n",
    "\n",
    "tokenizer.decode([  101,  1188,  1850, 22615,  2993,  1106,  1129,  3088,  1154,  3423,\n",
    "           119,   102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids tensor([   2,   48,  795, 6498, 2274,   20,   44, 2023,   77, 2491,    9,    3])\n",
      "token_type_ids tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "attention_mask tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] this sentense needs to be broken into pieces.[SEP]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v1\")\n",
    "\n",
    "raw_inputs = [\n",
    "    \"This sentense needs to be broken into pieces.\"\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "for k,v in inputs.items():\n",
    "    print (k,v[0])\n",
    "\n",
    "tokenizer.decode([   2,   48,  795, 6498, 2274,   20,   44, 2023,   77, 2491,    9,    3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches of inputs\n",
    "\n",
    "Usually models are fed with multiple sequences at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2023, 2003, 1037, 2200, 2146, 6251, 2021, 2009, 2515, 2025, 2298, 2066,\n",
      "         2009,  999]])\n",
      "tensor([[ 3.7695, -3.1442]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"This is a very long sentence but it does not look like it !\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "## Convert if only single sequence into array of inputs\n",
    "input_ids = torch.tensor([ids])\n",
    "print (input_ids)\n",
    "\n",
    "mout = model(input_ids)\n",
    "print (mout.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding / Attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a fountain pen.', 'This is a pen.']\n",
      "\n",
      " Ids without padding, (one token difference is observed.)\n",
      "[2023, 2003, 1037, 9545, 7279, 1012]\n",
      "[2023, 2003, 1037, 7279, 1012]\n",
      "\n",
      " After adding paddig token to the list.\n",
      "[2023, 2003, 1037, 9545, 7279, 1012]\n",
      "[2023, 2003, 1037, 7279, 1012, 0]\n",
      "\n",
      "Logits output by the model.\n",
      "tensor([[-3.0607,  3.2588],\n",
      "        [-0.1971,  0.4999]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Logits output by the model. (after attn mask)\n",
      "tensor([[-3.0607,  3.2588],\n",
      "        [ 2.3412, -2.1126]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_sequences = [\"This is a fountain pen.\", \n",
    "                   \"This is a pen.\"]\n",
    "\n",
    "print(input_sequences)\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "tokens = [tokenizer.tokenize(i) for i in input_sequences]\n",
    "ids = [tokenizer.convert_tokens_to_ids(i) for i in tokens]\n",
    "\n",
    "## Convert if only single sequence into array of inputs\n",
    "print (\"\\n Ids without padding, (one token difference is observed.)\")\n",
    "\n",
    "for k in ids:\n",
    "    print(k)\n",
    "\n",
    "ids[1].append(tokenizer.pad_token_id)\n",
    "\n",
    "print (\"\\n After adding paddig token to the list.\")\n",
    "\n",
    "for k in ids:\n",
    "    print(k)\n",
    "\n",
    "input_ids = torch.tensor(ids)\n",
    "\n",
    "mout = model(input_ids)\n",
    "print (\"\\nLogits output by the model.\")\n",
    "print (mout.logits)\n",
    "\n",
    "## padding token is considered as part of the sentense, hence we need to add an attention mask to avoid it.\n",
    "attn_mask = [[1,1,1,1,1,1],\n",
    "             [1,1,1,1,1,0]]\n",
    "\n",
    "mout = model(input_ids,attention_mask=torch.tensor(attn_mask))\n",
    "print (\"\\nLogits output by the model. (after attn mask)\")\n",
    "print (mout.logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QCML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
