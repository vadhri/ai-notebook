{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets\n",
        "!pip install -q transformers==4.30\n",
        "!pip install -q peft\n",
        "!pip install -q trl\n",
        "!pip install -q accelerate -U\n",
        "!pip install -q bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pViPYx6A_OX-",
        "outputId": "8ce3e7ca-4313-4873-af07-7e41b45debda"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.30\n",
            "  Using cached transformers-4.30.0-py3-none-any.whl (7.2 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30)\n",
            "  Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30) (2024.2.2)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.39.3\n",
            "    Uninstalling transformers-4.39.3:\n",
            "      Successfully uninstalled transformers-4.39.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "trl 0.8.1 requires transformers>=4.31.0, but you have transformers 4.30.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.13.3 transformers-4.30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrKYV81C-BYB",
        "outputId": "d351f802-b42b-4ff6-d3a3-e5ea62aaf56b"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
        "from transformers import AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig\n",
        "\n",
        "import torch"
      ],
      "metadata": {
        "id": "z-gZUhq6DGR3"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "kh_XwH0u_DYW"
      },
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "dataset = load_dataset(\"bitext/Bitext-customer-support-llm-chatbot-training-dataset\",  split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds = dataset.train_test_split(test_size=0.3)\n",
        "ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_f8oaT3TB6u",
        "outputId": "84eae37c-40bb-456f-c0ab-2fa676a04696"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['flags', 'instruction', 'category', 'intent', 'response'],\n",
              "        num_rows: 18810\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['flags', 'instruction', 'category', 'intent', 'response'],\n",
              "        num_rows: 8062\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/gdrive/MyDrive/llm/fine-tuned-fb-opt-125\",\n",
        "    dataloader_drop_last=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    num_train_epochs=20,\n",
        "    logging_steps=5,\n",
        "    per_device_train_batch_size=12,\n",
        "    per_device_eval_batch_size=12,\n",
        "    learning_rate=1e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_steps=100,\n",
        "    gradient_accumulation_steps=1,\n",
        "    gradient_checkpointing=False,\n",
        "    fp16=False,\n",
        "    weight_decay=0.05,\n",
        "    ddp_find_unused_parameters=False,\n",
        "    run_name=\"fb-opt-125\",\n",
        ")"
      ],
      "metadata": {
        "id": "sBwHv8UW_2v_"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "   load_in_8bit=True,\n",
        "   bnb_4bit_quant_type=\"fp4\",\n",
        "   bnb_4bit_use_double_quant=True,\n",
        "   bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\", quantization_config=nf4_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44lCBx8JCxRb",
        "outputId": "4fb85198-c654-45cc-f1b2-53c1a48fe940"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "  if param.ndim == 1:\n",
        "    param.data = param.data.to(torch.float16)\n",
        "\n",
        "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
        "model.enable_input_require_grads()\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "  def forward(self, x): return super().forward(x).to(torch.float16)\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)"
      ],
      "metadata": {
        "id": "-6H7MwYwDOAY"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "eQnLJ-lBkkeA"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YXg0cVkjKQH",
        "outputId": "1e52a765-e7a2-4d00-b209-0493aa17ccc8"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 589,824 || all params: 125,829,120 || trainable%: 0.46875\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): PeftModelForCausalLM(\n",
              "      (base_model): LoraModel(\n",
              "        (model): OPTForCausalLM(\n",
              "          (model): OPTModel(\n",
              "            (decoder): OPTDecoder(\n",
              "              (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
              "              (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
              "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (layers): ModuleList(\n",
              "                (0-11): 12 x OPTDecoderLayer(\n",
              "                  (self_attn): OPTAttention(\n",
              "                    (k_proj): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                    (v_proj): lora.Linear8bitLt(\n",
              "                      (base_layer): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                      (lora_dropout): ModuleDict(\n",
              "                        (default): Dropout(p=0.05, inplace=False)\n",
              "                      )\n",
              "                      (lora_A): ModuleDict(\n",
              "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                      )\n",
              "                      (lora_B): ModuleDict(\n",
              "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                      )\n",
              "                      (lora_embedding_A): ParameterDict()\n",
              "                      (lora_embedding_B): ParameterDict()\n",
              "                    )\n",
              "                    (q_proj): lora.Linear8bitLt(\n",
              "                      (base_layer): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                      (lora_dropout): ModuleDict(\n",
              "                        (default): Dropout(p=0.05, inplace=False)\n",
              "                      )\n",
              "                      (lora_A): ModuleDict(\n",
              "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                      )\n",
              "                      (lora_B): ModuleDict(\n",
              "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                      )\n",
              "                      (lora_embedding_A): ParameterDict()\n",
              "                      (lora_embedding_B): ParameterDict()\n",
              "                    )\n",
              "                    (out_proj): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
              "                  )\n",
              "                  (activation_fn): ReLU()\n",
              "                  (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                  (fc1): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
              "                  (fc2): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
              "                  (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (lm_head): CastOutputToFloat(\n",
              "            (0): Linear(in_features=768, out_features=50272, bias=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from trl.trainer import ConstantLengthDataset\n",
        "\n",
        "def formatting_func(example):\n",
        "    # print (\"formatting_func : \", example)\n",
        "    text = f\"### Question: {example['instruction']}\\n  ### Category: {example['category']}\\n ### Answer: {example['response']}\"\n",
        "    return text\n",
        "\n",
        "train_dataset = ConstantLengthDataset(\n",
        "    tokenizer,\n",
        "    ds['train'],\n",
        "    formatting_func=formatting_func,\n",
        "    seq_length=1024\n",
        ")\n",
        "\n",
        "eval_dataset = ConstantLengthDataset(\n",
        "    tokenizer,\n",
        "    ds['test'],\n",
        "    formatting_func=formatting_func,\n",
        "    seq_length=1024\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    max_seq_length=1024,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    peft_config=lora_config,\n",
        "    packing=True,\n",
        ")\n",
        "\n",
        "print(\"Training...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "id": "1Id6wuTjD7YB",
        "outputId": "7b0495d6-f63d-4b53-92c0-fd4dbe56fab9"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4800' max='31360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 4800/31360 1:22:52 < 7:38:45, 0.96 it/s, Epoch 19/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.800600</td>\n",
              "      <td>1.709961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.601000</td>\n",
              "      <td>1.482422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.449200</td>\n",
              "      <td>1.363281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.373000</td>\n",
              "      <td>1.291016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.306800</td>\n",
              "      <td>1.241211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.316000</td>\n",
              "      <td>1.216797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.324800</td>\n",
              "      <td>1.210938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.251000</td>\n",
              "      <td>1.177734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.244300</td>\n",
              "      <td>1.150391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.193400</td>\n",
              "      <td>1.130859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.172100</td>\n",
              "      <td>1.112305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.180500</td>\n",
              "      <td>1.107422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.189100</td>\n",
              "      <td>1.095703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.149800</td>\n",
              "      <td>1.069336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.166800</td>\n",
              "      <td>1.060547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.118000</td>\n",
              "      <td>1.051758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.102500</td>\n",
              "      <td>1.041016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.140200</td>\n",
              "      <td>1.044922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.102000</td>\n",
              "      <td>1.036133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.116600</td>\n",
              "      <td>1.025391</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=4800, training_loss=1.2973309326171876, metrics={'train_runtime': 4976.2802, 'train_samples_per_second': 75.599, 'train_steps_per_second': 6.302, 'total_flos': 3.022538422616064e+16, 'train_loss': 1.2973309326171876, 'epoch': 19.15})"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "  \"facebook/opt-125m\", return_dict=True, torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load the LoRA adaptors\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, \"/content/gdrive/MyDrive/llm/fine-tuned-fb-opt-125/checkpoint-4800/\")\n",
        "model.eval()\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Save for future use\n",
        "model.save_pretrained(\"/content/gdrive/MyDrive/llm/fine-tuned-fb-opt-125/merged\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "wjaWoXJT13cM"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"I am trying to cancel a purchase order no. 1234.\", return_tensors=\"pt\").to(device)\n",
        "generation_output = model.generate(**inputs,\n",
        "                                    return_dict_in_generate=True,\n",
        "                                    output_scores=True,\n",
        "                                    max_length=100,\n",
        "                                    num_beams=1,\n",
        "                                    do_sample=True,\n",
        "                                    repetition_penalty=1.5,\n",
        "                                    length_penalty=2.)"
      ],
      "metadata": {
        "id": "QvLx-gxH2SUz"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_output\n",
        "print( tokenizer.decode(generation_output['sequences'][0]) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGNgWDVt2t8G",
        "outputId": "30da34f0-f797-4743-da36-24d5d43d4b2d"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "</s>I am trying to cancel a purchase order no. 1234. Is there an option for me? That was the last time I went and it has been getting cancelled too :(\n",
            "Just go do this!  You can get them at any WalMart, Target OR your local grocery store - but that is on top of their clearance specials!! :) Just look under my items & search \"Walmart offers discounts,\" which you would find if anyone actually looked up what they are giving away/affiliate\n"
          ]
        }
      ]
    }
  ]
}