{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "NbIyx9PFXgL3",
      "metadata": {
        "id": "NbIyx9PFXgL3"
      },
      "source": [
        "# Decoder only model\n",
        "The following implementation uses word embeddings using nltk tokenizer for words and tries to use the multi headed self attention model to train on text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c1dd7e1c-b4d8-460d-8376-2bb2b16e3290",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1dd7e1c-b4d8-460d-8376-2bb2b16e3290",
        "outputId": "41ee8ac0-88b1-4d27-f190-b79984b40c50",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import nltk\n",
        "import torch.nn as nn\n",
        "from random import randrange\n",
        "import torch.nn.functional as F\n",
        "nltk.download('punkt')\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print (device)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5b358caa-01f5-48d7-90f9-7a4ce441b351",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "5b358caa-01f5-48d7-90f9-7a4ce441b351",
        "outputId": "307144a3-125c-434f-d2dd-5414269941ae",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>\n",
              "  table {margin-left: 0 !important;}\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%html\n",
        "<style>\n",
        "  table {margin-left: 0 !important;}\n",
        "</style>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90b30589-98a2-475f-a35f-3b1b44456bfd",
      "metadata": {
        "id": "90b30589-98a2-475f-a35f-3b1b44456bfd",
        "tags": []
      },
      "source": [
        "## Tokenizing input data\n",
        "\n",
        "Create a mapping of the tokenized words into text and viceversa.\n",
        "\n",
        "- load_and_encode_data_nltk\n",
        "\n",
        "nltk uses the nltk tokenizer to split the text into sentenses and words. \\\n",
        "https://www.nltk.org/api/nltk.tokenize.html\n",
        "\n",
        "- tiktoken\n",
        "[todo]\n",
        "- sentence piece\n",
        "[todo]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9083b6e1-2313-40d3-adbe-c7506cd28713",
      "metadata": {
        "id": "9083b6e1-2313-40d3-adbe-c7506cd28713",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def read_data(filename):\n",
        "    text = None\n",
        "    with open(filename, \"r\", encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6b89bc16-5ee1-4441-8f2d-55afae7c880c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b89bc16-5ee1-4441-8f2d-55afae7c880c",
        "outputId": "2a46c6e2-263e-4962-b509-07380128fcfe",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of dataset =  1013924\n"
          ]
        }
      ],
      "source": [
        "def load_and_encode_data_nltk(filename):\n",
        "    word_to_lookup = {}\n",
        "    lookup_to_word = {}\n",
        "    encoded_data = []\n",
        "\n",
        "    data = read_data(filename)\n",
        "    print ('length of dataset = ', len(data))\n",
        "    tokenized_data = [word_tokenize(w) for w in [s for s in sent_tokenize(data)]]\n",
        "    vocabulary = set()\n",
        "\n",
        "    for s in tokenized_data:\n",
        "        for w in s:\n",
        "            vocabulary.add(w)\n",
        "\n",
        "    vocabulary = sorted(vocabulary)\n",
        "\n",
        "    for c, i in list(zip(vocabulary, range(len(vocabulary)))):\n",
        "        word_to_lookup[c] = i\n",
        "        lookup_to_word[i] = c\n",
        "\n",
        "    for s in tokenized_data:\n",
        "        for w in s:\n",
        "            encoded_data.append(word_to_lookup[w])\n",
        "\n",
        "    return word_to_lookup, lookup_to_word, encoded_data\n",
        "\n",
        "w2l, l2w, tokenized_data = load_and_encode_data_nltk(\"/content/drive/MyDrive/colab/pg1400.txt\")\n",
        "\n",
        "tensor_tokenized_data = torch.tensor(tokenized_data, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b6a83ba6-a764-470f-8d75-a7c8974c9f5f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6a83ba6-a764-470f-8d75-a7c8974c9f5f",
        "outputId": "40f16e4a-68a5-438b-bb75-df86c92534c8",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([229192]) torch.int64\n",
            "tensor([13697,  1241,   661,  5082,  8781,   647,   537,  1537,  5120,  7431,\n",
            "         5967, 12111, 12807,  8781,  2302,  2307,  7092, 12111,  1607,  1453,\n",
            "         2246,  8407,  8902,  9083,  8781, 12111, 13380,  2509,  8622,  4109,\n",
            "         2246, 13322,  2188,  8622, 10315, 13172,    10,  1779,  8135,  4077,\n",
            "         7440,     8,  6277,  7440,  2584,  8867,  9937,  7440, 12617, 12111,\n",
            "        12069,  8781, 12111,  1241,   661,   904,  7119, 13322, 12169,  5120,\n",
            "         8867,  8837,  2509, 13444,    10,   770, 13477,  2388,  8660,  7883,\n",
            "         7092, 12111,  1607,  1453,     8, 13477, 13265,  6661, 12279,  3453,\n",
            "        12111,  7693,  8781, 12111,  4137, 13181, 13477,  2388,  7883,  2752,\n",
            "        12817, 12169,  5082,    10,  1554,    56,   647,   537,   162,    56])\n"
          ]
        }
      ],
      "source": [
        "print(tensor_tokenized_data.shape, tensor_tokenized_data.dtype)\n",
        "print(tensor_tokenized_data[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a57ea747-587e-4228-8036-333d1625bb30",
      "metadata": {
        "id": "a57ea747-587e-4228-8036-333d1625bb30"
      },
      "source": [
        "## Test training dataset split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b85455ca-6b37-4b29-a318-7efbc518c96e",
      "metadata": {
        "id": "b85455ca-6b37-4b29-a318-7efbc518c96e",
        "tags": []
      },
      "outputs": [],
      "source": [
        "n = int(0.8*len(tensor_tokenized_data))\n",
        "train_data = tensor_tokenized_data[:n]\n",
        "test_data = tensor_tokenized_data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "917c5bb5-004e-4b7e-b0e2-ed13a686dda9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "917c5bb5-004e-4b7e-b0e2-ed13a686dda9",
        "outputId": "10586154-6ae2-4030-9dcf-a9d0456cb627",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([13697,  1241,   661,  ...,     8,  2246, 13181]),\n",
              " tensor([12111, 11712, 12997,  ...,  8589,  5083,    10]))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data, test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e1aa9af7-b3e5-4a38-ad56-e0df654ef415",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1aa9af7-b3e5-4a38-ad56-e0df654ef415",
        "outputId": "fa3c2526-dbad-4219-afc2-a11bb449e9bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([13697,  1241,   661,  5082,  8781,   647,   537,  1537,  5120,  7431,\n",
              "         5967, 12111, 12807,  8781,  2302,  2307,  7092, 12111,  1607,  1453,\n",
              "         2246,  8407,  8902,  9083,  8781, 12111, 13380,  2509,  8622,  4109,\n",
              "         2246, 13322,  2188,  8622, 10315, 13172,    10,  1779,  8135,  4077,\n",
              "         7440,     8,  6277,  7440,  2584,  8867,  9937,  7440, 12617, 12111,\n",
              "        12069,  8781, 12111,  1241,   661,   904,  7119, 13322, 12169,  5120,\n",
              "         8867,  8837,  2509, 13444,    10,   770, 13477,  2388,  8660,  7883,\n",
              "         7092, 12111,  1607,  1453,     8, 13477, 13265,  6661, 12279,  3453,\n",
              "        12111,  7693,  8781, 12111,  4137, 13181, 13477,  2388,  7883,  2752,\n",
              "        12817, 12169,  5082,    10,  1554,    56,   647,   537,   162,    56,\n",
              "          321,   441,  1301,  4336,    56,   847,    11,     8,    36,  1785,\n",
              "         5082,     1,    33,  1786,  1026,  9989, 12780,    56,  1109,    51,\n",
              "            8,    40,   891,    56,   505,   395,    56,   122,   126,  1628,\n",
              "         2246,   421,  1695,     7,     7,     7,  1349,  1101,  1490,  1153,\n",
              "          598,   478,   597,   484,     7,     7,     7,  1785,   771,  1786,\n",
              "          647,   537,  1785,    35,   494,  1786,  3235,   321,   441,   375,\n",
              "          320,   753,    10,   320,   756,    10,   320,   757,    10,   320,\n",
              "          767,    10,   320,  1618,   320,  1620,    10,   320,  1621,    10,\n",
              "          320,  1622,    10,   320,   768,    10,   320,  1726,    10,   320,\n",
              "         1727,    10,   320,  1728,    10,   320,  1729,    10,   320,  1730,\n",
              "           10,   320,  1743,    10,   320,  1744,    10,   320,  1745,    10,\n",
              "          320,  1746,    10,   320,  1731,    10,   320,  1747,    10,   320,\n",
              "         1748,    10,   320,  1749,    10,   320,  1750,    10,   320,  1751,\n",
              "           10,   320,  1753,    10,   320,  1754,    10,   320,  1755,    10,\n",
              "          320,  1756,    10,   320,  1752,    10,   320,  1757,    10,   320,\n",
              "         1758,    10,   320,  1759,    10,   320,  1760])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "no_of_embeddings = 384 ## Number of trainable parameters per token.\n",
        "attention_head_size = 16\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "no_of_heads = 6\n",
        "evaluation_iters = 200\n",
        "total_iters = 5000\n",
        "dropout = 0.2 # to prevent overfitting\n",
        "\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "60b9bc71-ec56-4f12-b607-a1ba866374e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "60b9bc71-ec56-4f12-b607-a1ba866374e8",
        "outputId": "8958b44c-afe2-436c-f182-831e3fe899a5",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Project Gutenberg eBook of Great Expectations'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\" \".join([l2w[i] for i in train_data[1:7].numpy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1b2df35b-3e51-4a7e-b108-22aefba93fc8",
      "metadata": {
        "id": "1b2df35b-3e51-4a7e-b108-22aefba93fc8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def get_data_for_processing(training_data_set):\n",
        "    data = train_data if training_data_set else test_data\n",
        "    sample_indices = torch.randint(len(data)-block_size, (batch_size, ))\n",
        "    x = torch.stack([data[i:i+block_size] for i in sample_indices])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in sample_indices])\n",
        "    return x.to(device), y.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7957b67f-0831-445a-9b8f-8a60fd9f0d30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7957b67f-0831-445a-9b8f-8a60fd9f0d30",
        "outputId": "77a18580-758a-485e-d5cb-ce24e8c3234b",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "and said\n",
            "tapping the\n",
            "left in\n",
            "make me\n",
            "my eyes\n",
            "‘ And\n",
            "merely stopping\n",
            "that I\n",
            "there !\n",
            "in great\n",
            ", and\n",
            "the forehead\n",
            "by your\n",
            "how poor\n",
            "of the\n",
            "“ Yes\n",
            "As I\n",
            "Joe ?\n",
            "and said\n",
            "there were\n",
            "soldiers .\n",
            "dead sister\n",
            "believe he\n",
            "was many\n",
            "the man\n",
            "t got\n",
            "were some\n",
            "girl is\n",
            "on my\n",
            "beast .\n",
            "eyes ,\n",
            "’ She\n",
            "or vagrants\n",
            "Biddy to\n",
            "_that_ would\n",
            "on no\n",
            "one occasion\n",
            ", and\n",
            "glad to\n",
            "secret from\n",
            "! I\n",
            "forge ,\n",
            "And how\n",
            "the boy\n",
            "his attention\n",
            "I took\n",
            "believe ,\n",
            "of hearing\n",
            "until at\n",
            "objected to\n",
            "was not\n",
            "downstairs .\n",
            "speak yet\n",
            "tell me\n",
            "was going\n",
            "my indignant\n",
            "here ,\n",
            "extent ,\n",
            "sir !\n",
            "cleaning up\n",
            "hope ,\n",
            "the right\n",
            "inaccessibility which\n",
            "! ”\n",
            "13698\n"
          ]
        }
      ],
      "source": [
        "samples_x, samples_y = get_data_for_processing(True)\n",
        "for batch in range(batch_size):\n",
        "    for block in range(1):\n",
        "        context = samples_x[batch, :block+1]\n",
        "        target = samples_y[batch, block]\n",
        "        print (\" \".join([l2w[i] for i in context.cpu().numpy()]), l2w[target.item()])\n",
        "\n",
        "vocab_size = len(l2w)\n",
        "print (vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nONbUXDVY_rm",
      "metadata": {
        "id": "nONbUXDVY_rm"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "N64B5mRKCEu_",
      "metadata": {
        "id": "N64B5mRKCEu_"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimation_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'test']:\n",
        "    losses = torch.zeros(evaluation_iters)\n",
        "    for k in range(evaluation_iters):\n",
        "      X,Y = get_data_for_processing(True if split == 'train' else False)\n",
        "      logits, loss = model(X,Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xvEa2SiiZZwp",
      "metadata": {
        "id": "xvEa2SiiZZwp"
      },
      "source": [
        "## Attention blocks implentation.\n",
        "The various classes below signify the implmenetation from various parts of the attention model for decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KRGr_rpcZZvP",
      "metadata": {
        "id": "KRGr_rpcZZvP"
      },
      "source": [
        "### Self-attention head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "xwZG0wUpknSw",
      "metadata": {
        "id": "xwZG0wUpknSw"
      },
      "outputs": [],
      "source": [
        "class SelfAttentionHead(nn.Module):\n",
        "  def __init__(self, attention_head_size):\n",
        "    super(SelfAttentionHead, self).__init__()\n",
        "    self.key = nn.Linear(no_of_embeddings, attention_head_size, bias=False)\n",
        "    self.query = nn.Linear(no_of_embeddings, attention_head_size, bias=False)\n",
        "    self.value = nn.Linear(no_of_embeddings, attention_head_size, bias=False)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "  def forward(self, idx):\n",
        "    bts, bs, no_of_embeddings = idx.shape\n",
        "    # print ('idx = ', idx.shape)\n",
        "    k = self.key(idx)\n",
        "    q = self.query(idx)\n",
        "\n",
        "    weights = q @ k.transpose(-2,-1) * no_of_embeddings**-0.5\n",
        "    # print ('w after transpose mul= ', weights.shape)\n",
        "    weights = weights.masked_fill(self.tril[:bs][:bs] == 0, float('-inf'))\n",
        "\n",
        "    weights = F.softmax(weights, dim=-1)\n",
        "    weights = self.dropout(weights)\n",
        "\n",
        "    values = self.value(idx)\n",
        "    # print (weights.shape, values.shape)\n",
        "\n",
        "    out = weights @ values\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kmuLO6DLZscP",
      "metadata": {
        "id": "kmuLO6DLZscP"
      },
      "source": [
        "### Multi attention head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "Qv4jpiBz-C8i",
      "metadata": {
        "id": "Qv4jpiBz-C8i"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttn(nn.Module):\n",
        "  def __init__(self, num_heads, attention_head_size):\n",
        "    super(MultiHeadAttn, self).__init__()\n",
        "    self.heads = nn.ModuleList([SelfAttentionHead(attention_head_size) for _ in range(num_heads)])\n",
        "    self.self_projection = nn.Linear(no_of_embeddings, no_of_embeddings)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self, idx):\n",
        "    self_attn_out = torch.cat([head(idx) for head in self.heads], dim=-1)\n",
        "    return self.dropout(self.self_projection(self_attn_out))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Dns3OYKxZvvF",
      "metadata": {
        "id": "Dns3OYKxZvvF"
      },
      "source": [
        "### Feed forward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1aMePhR2BOZf",
      "metadata": {
        "id": "1aMePhR2BOZf"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, no_of_embeddings):\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.network = nn.Sequential(\n",
        "        nn.Linear(no_of_embeddings, 4*no_of_embeddings),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*no_of_embeddings, no_of_embeddings), # This is projection.\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, idx):\n",
        "    return self.network(idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EkaJggNXZ3KI",
      "metadata": {
        "id": "EkaJggNXZ3KI"
      },
      "source": [
        "### Block\n",
        "General block structure to hold other pieces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "Oa0tB0xCCnf_",
      "metadata": {
        "id": "Oa0tB0xCCnf_"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, no_of_embeddings, no_of_heads):\n",
        "    super(Block, self).__init__()\n",
        "    self.self_attn_heads = MultiHeadAttn(no_of_heads, no_of_embeddings//no_of_heads)\n",
        "    self.feed_forward = FeedForward(no_of_embeddings)\n",
        "    self.layer_norm1 = nn.LayerNorm(no_of_embeddings)\n",
        "    self.layer_norm2 = nn.LayerNorm(no_of_embeddings)\n",
        "\n",
        "  # idx is added as a concept from residual image nets to preserve the original x input.\n",
        "  def forward(self, idx):\n",
        "    self_attn_heads_out = idx + self.self_attn_heads(self.layer_norm1(idx))\n",
        "    ffwd_out = idx + self.feed_forward(self.layer_norm2(self_attn_heads_out))\n",
        "    return ffwd_out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u68n1pqxZ-VX",
      "metadata": {
        "id": "u68n1pqxZ-VX"
      },
      "source": [
        "### Main model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_s2NQhkZZ-RD",
      "metadata": {
        "id": "_s2NQhkZZ-RD"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "da43ee4c-974b-494c-b83e-5cbf3081eaa2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da43ee4c-974b-494c-b83e-5cbf3081eaa2",
        "outputId": "a1da5191-a5a3-43d6-ae08-6fbe66ccb8be",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16384, 13698]) tensor(9.6857, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "class LLM(nn.Module):\n",
        "    def __init__(self, vocab_size, context, embedded_dim):\n",
        "      super(LLM, self).__init__()\n",
        "      self.embedding = nn.Embedding(vocab_size, no_of_embeddings) # batches, block_size, embeddings\n",
        "      self.position_embedding = nn.Embedding(block_size, no_of_embeddings) # block_size, embeddings\n",
        "\n",
        "      self.blocks = nn.Sequential(\n",
        "          Block(no_of_embeddings, no_of_heads),\n",
        "          Block(no_of_embeddings, no_of_heads),\n",
        "          Block(no_of_embeddings, no_of_heads),\n",
        "          Block(no_of_embeddings, no_of_heads),\n",
        "          Block(no_of_embeddings, no_of_heads),\n",
        "          Block(no_of_embeddings, no_of_heads),\n",
        "          nn.LayerNorm(no_of_embeddings)\n",
        "      )\n",
        "      self.language_model_head = nn.Linear(no_of_embeddings, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "      batch, blocksize = idx.shape\n",
        "      token_embedding = self.embedding(idx)\n",
        "      block_size_values = torch.arange(blocksize, device=device)\n",
        "      position_embedding = self.position_embedding(block_size_values)\n",
        "      tokpos_embedding = token_embedding + position_embedding\n",
        "      # print (tokpos_embedding.shape, token_embedding.shape, position_embedding.shape)\n",
        "      self_attn_out = self.blocks(tokpos_embedding)\n",
        "      logits = self.language_model_head(self_attn_out)\n",
        "\n",
        "      if targets is None:\n",
        "          loss = None\n",
        "      else:\n",
        "          B,T,C = logits.shape\n",
        "          logits = logits.view(B*T,C)\n",
        "          targets = targets.view(B*T)\n",
        "          loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "      return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "      for _ in range(max_new_tokens):\n",
        "          idx_condition = idx[:, -block_size:]\n",
        "          # print (idx_condition)\n",
        "          logits, loss = self(idx_condition)\n",
        "          logits = logits[:, -1,  :]\n",
        "          probs = F.softmax(logits, dim=-1)\n",
        "          idx_next = torch.multinomial(probs, num_samples=1)\n",
        "          idx = torch.cat((idx, idx_next), dim=1)\n",
        "      return idx\n",
        "\n",
        "model = LLM(vocab_size, block_size, vocab_size).to(device)\n",
        "logits, loss = model(samples_x, samples_y)\n",
        "print (logits.shape, loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "0b544b4e-c706-4cd2-8916-bf07c55300a0",
      "metadata": {
        "id": "0b544b4e-c706-4cd2-8916-bf07c55300a0"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9wVxzolBaDvS",
      "metadata": {
        "id": "9wVxzolBaDvS"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6rg0IyXGaDeT",
      "metadata": {
        "id": "6rg0IyXGaDeT"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "4-tjz64nAEA4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-tjz64nAEA4",
        "outputId": "f79a34f3-b549-425e-ddfd-e4fe838554ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data loss =  9.685654640197754 Test data loss =  9.68315315246582\n",
            "Training data loss =  4.157632827758789 Test data loss =  5.438475131988525\n",
            "Training data loss =  3.4296507835388184 Test data loss =  5.6463623046875\n",
            "Training data loss =  2.558993339538574 Test data loss =  6.084308624267578\n",
            "Training data loss =  1.604283094406128 Test data loss =  6.748580455780029\n",
            "Training data loss =  0.9546141028404236 Test data loss =  7.504760265350342\n",
            "Training data loss =  0.5714260935783386 Test data loss =  8.206672668457031\n",
            "Training data loss =  0.38156673312187195 Test data loss =  8.755910873413086\n",
            "Training data loss =  0.2747141420841217 Test data loss =  9.267433166503906\n",
            "Training data loss =  0.21708902716636658 Test data loss =  9.593343734741211\n",
            "Training data loss =  0.18563181161880493 Test data loss =  9.927550315856934\n",
            "Training data loss =  0.16241146624088287 Test data loss =  10.208747863769531\n",
            "Training data loss =  0.1444084495306015 Test data loss =  10.530871391296387\n",
            "Training data loss =  0.13475626707077026 Test data loss =  10.752009391784668\n",
            "Training data loss =  0.12396232783794403 Test data loss =  11.015612602233887\n",
            "Training data loss =  0.1186148151755333 Test data loss =  11.24975872039795\n",
            "Training data loss =  0.11129239946603775 Test data loss =  11.465100288391113\n",
            "Training data loss =  0.1064680963754654 Test data loss =  11.589022636413574\n",
            "Training data loss =  0.10141730308532715 Test data loss =  11.749783515930176\n",
            "Training data loss =  0.09700805693864822 Test data loss =  11.956368446350098\n",
            "Training data loss =  0.09456402063369751 Test data loss =  12.08425521850586\n",
            "Training data loss =  0.09139929711818695 Test data loss =  12.244007110595703\n",
            "Training data loss =  0.08866949379444122 Test data loss =  12.409717559814453\n",
            "Training data loss =  0.08616556972265244 Test data loss =  12.498862266540527\n",
            "Training data loss =  0.08424943685531616 Test data loss =  12.613716125488281\n"
          ]
        }
      ],
      "source": [
        "for step in range(total_iters):\n",
        "    if step % evaluation_iters == 0:\n",
        "      losses = estimation_loss()\n",
        "      print(\"Training data loss = \", losses['train'].item(), \"Test data loss = \", losses['test'].item())\n",
        "    samples_x, samples_y = get_data_for_processing(True)\n",
        "    logits, loss = model(samples_x, samples_y)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HfkWRALNaF1M",
      "metadata": {
        "id": "HfkWRALNaF1M"
      },
      "source": [
        "## Example generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "xJxKeP9TAF9P",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJxKeP9TAF9P",
        "outputId": "9962a23f-d000-42f3-c914-a9741d30627d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INPUT sequence (256 chars)\n",
            ", I ’ ll bet you. ” He was gobbling mincemeat , meatbone , bread , cheese , and pork pie , all at once : staring distrustfully while he did so at the mist all round us , and often stopping—even stopping his jaws—to listen . Some real or fancied sound , some clink upon the river or breathing of beast upon the marsh , now gave him a start , and he said , suddenly , — “ You ’ re not a deceiving imp ? You brought no one with you ? ” “ No , sir ! No ! ” “ Nor giv ’ no one the office to follow you ? ” “ No ! ” “ Well , ” said he , “ I believe you . You ’ d be but a fierce young hound indeed , if at your time of life you could help to hunt a wretched warmint hunted as near death and dunghill as this poor wretched warmint is ! ” Something clicked in his throat as if he had works in him like a clock , and was going to strike . And he smeared his ragged rough sleeve over his eyes . Pitying his desolation , and watching him as he gradually settled down upon the pie , I made bold to say , “ I am glad you enjoy it. ” “ Did you speak ? ” “ I said I was glad you enjoyed it. ” “ Thankee , my boy\n",
            "Output generated (256 chars)\n",
            ". I do. ” I had told him then I told him if he would have done turning a look like , and he would be best trouble to do it . But then , and I believe it ’ s with a ragged book about when ten and I was dead , and I swore to myself to have been brought here and die happy. ” “ bound , ” said the slate as if he respectfully bent down , “ and they are wanted to look they are ! ” “ Did you , Joe ? ” “ No , ” said Joe , glancing over the fire , “ he ’ m sure you have done the boy , and you knowed her about that kind of lies , and the hand anywhere else ? ” “ You don ’ t mean to Mrs. Joe , ” I added , “ I ’ ll believe , in some time fingers , and God bless to Mrs. J. Gargery , and I ’ ll soon go. ” when Mrs. Joe stopped , Joe stopped me , and he throwing an exultant face . “ I know ’ ll work him. ” “ Joe resumed , when I had unfixed his eye fell on that Sunday dress , and got home and got home . We were then we must break out together while it up , pretty well. ” My sister catching Joe , and in conference when it could be blind monster ,\n"
          ]
        }
      ],
      "source": [
        "g = model.generate(torch.tensor([tokenized_data[8000:8256]], dtype=torch.long, device=device), max_new_tokens=256)\n",
        "\n",
        "print (\"INPUT sequence (256 chars)\")\n",
        "print (\" \".join([l2w[i] for i in tokenized_data[8000:8256]]))\n",
        "\n",
        "print (\"Output generated (256 chars)\")\n",
        "print(\" \".join([l2w[i] for i in g.cpu().numpy()[0][256:]]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yjFR2DS5nwCu",
      "metadata": {
        "id": "yjFR2DS5nwCu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
