{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c1dd7e1c-b4d8-460d-8376-2bb2b16e3290",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vratnam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b30589-98a2-475f-a35f-3b1b44456bfd",
   "metadata": {
    "tags": []
   },
   "source": [
    "Create a mapping of the tokenized words into text and viceversa.\n",
    "\n",
    "- load_and_encode_data_nltk\n",
    "\n",
    "nltk uses the nltk tokenizer to split the text into sentenses and words. \\\n",
    "https://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "- tiktoken\n",
    "[todo]\n",
    "- sentence piece\n",
    "[todo] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9083b6e1-2313-40d3-adbe-c7506cd28713",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    text = None\n",
    "    with open(filename, \"r\", encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6b89bc16-5ee1-4441-8f2d-55afae7c880c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset =  1013924\n"
     ]
    }
   ],
   "source": [
    "def load_and_encode_data_nltk(filename):\n",
    "    word_to_lookup = {}\n",
    "    lookup_to_word = {}\n",
    "    encoded_data = []\n",
    "\n",
    "    data = read_data(filename)\n",
    "    print ('length of dataset = ', len(data))    \n",
    "    tokenized_data = [word_tokenize(w) for w in [s for s in sent_tokenize(data)]]\n",
    "    vocabulary = set()\n",
    "\n",
    "    for s in tokenized_data:\n",
    "        for w in s:\n",
    "            vocabulary.add(w)\n",
    "\n",
    "    vocabulary = sorted(vocabulary)\n",
    "\n",
    "    for c, i in list(zip(vocab, range(len(vocab)))):\n",
    "        word_to_lookup[c] = i\n",
    "        lookup_to_word[i] = c\n",
    "\n",
    "    for s in tokenized_data:\n",
    "        for w in s:\n",
    "            encoded_data.append(word_to_lookup[w])        \n",
    "        \n",
    "    return word_to_lookup, lookup_to_word, encoded_data\n",
    "\n",
    "w2l, l2w, tokenized_data = load_and_encode_data_nltk(\"data/pg1400.txt\")\n",
    "\n",
    "tensor_tokenized_data = torch.tensor(tokenized_data, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b6a83ba6-a764-470f-8d75-a7c8974c9f5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([229192]) torch.int64\n",
      "tensor([13697,  1241,   661,  5082,  8781,   647,   537,  1537,  5120,  7431,\n",
      "         5967, 12111, 12807,  8781,  2302,  2307,  7092, 12111,  1607,  1453,\n",
      "         2246,  8407,  8902,  9083,  8781, 12111, 13380,  2509,  8622,  4109,\n",
      "         2246, 13322,  2188,  8622, 10315, 13172,    10,  1779,  8135,  4077,\n",
      "         7440,     8,  6277,  7440,  2584,  8867,  9937,  7440, 12617, 12111,\n",
      "        12069,  8781, 12111,  1241,   661,   904,  7119, 13322, 12169,  5120,\n",
      "         8867,  8837,  2509, 13444,    10,   770, 13477,  2388,  8660,  7883,\n",
      "         7092, 12111,  1607,  1453,     8, 13477, 13265,  6661, 12279,  3453,\n",
      "        12111,  7693,  8781, 12111,  4137, 13181, 13477,  2388,  7883,  2752,\n",
      "        12817, 12169,  5082,    10,  1554,    56,   647,   537,   162,    56])\n"
     ]
    }
   ],
   "source": [
    "print(tensor_tokenized_data.shape, tensor_tokenized_data.dtype)\n",
    "print(tensor_tokenized_data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57ea747-587e-4228-8036-333d1625bb30",
   "metadata": {},
   "source": [
    "Test training dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b85455ca-6b37-4b29-a318-7efbc518c96e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = int(0.8*len(tensor_tokenized_data))\n",
    "train_data = tensor_tokenized_data[:n]\n",
    "test_data = tensor_tokenized_data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "917c5bb5-004e-4b7e-b0e2-ed13a686dda9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([13697,  1241,   661,  ...,     8,  2246, 13181]),\n",
       " tensor([12111, 11712, 12997,  ...,  8589,  5083,    10]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423bd3f0-e82a-42ed-a246-3ac9e6b69b43",
   "metadata": {},
   "source": [
    "--------- EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aa9af7-b3e5-4a38-ad56-e0df654ef415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
