{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c1dd7e1c-b4d8-460d-8376-2bb2b16e3290",
      "metadata": {
        "tags": [],
        "id": "c1dd7e1c-b4d8-460d-8376-2bb2b16e3290",
        "outputId": "432b7356-6b8e-4ade-fd63-01bad3749db4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import nltk\n",
        "from random import randrange\n",
        "nltk.download('punkt')\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5b358caa-01f5-48d7-90f9-7a4ce441b351",
      "metadata": {
        "tags": [],
        "id": "5b358caa-01f5-48d7-90f9-7a4ce441b351",
        "outputId": "94ecff9a-0758-414a-e859-8f5bf78faf16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "  table {margin-left: 0 !important;}\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "%%html\n",
        "<style>\n",
        "  table {margin-left: 0 !important;}\n",
        "</style>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90b30589-98a2-475f-a35f-3b1b44456bfd",
      "metadata": {
        "tags": [],
        "id": "90b30589-98a2-475f-a35f-3b1b44456bfd"
      },
      "source": [
        "## Tokenizing input data\n",
        "\n",
        "Create a mapping of the tokenized words into text and viceversa.\n",
        "\n",
        "- load_and_encode_data_nltk\n",
        "\n",
        "nltk uses the nltk tokenizer to split the text into sentenses and words. \\\n",
        "https://www.nltk.org/api/nltk.tokenize.html\n",
        "\n",
        "- tiktoken\n",
        "[todo]\n",
        "- sentence piece\n",
        "[todo]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9083b6e1-2313-40d3-adbe-c7506cd28713",
      "metadata": {
        "tags": [],
        "id": "9083b6e1-2313-40d3-adbe-c7506cd28713"
      },
      "outputs": [],
      "source": [
        "def read_data(filename):\n",
        "    text = None\n",
        "    with open(filename, \"r\", encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6b89bc16-5ee1-4441-8f2d-55afae7c880c",
      "metadata": {
        "tags": [],
        "id": "6b89bc16-5ee1-4441-8f2d-55afae7c880c",
        "outputId": "e7caf09b-d163-492a-b0a9-14fe36617436",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset =  1013924\n"
          ]
        }
      ],
      "source": [
        "def load_and_encode_data_nltk(filename):\n",
        "    word_to_lookup = {}\n",
        "    lookup_to_word = {}\n",
        "    encoded_data = []\n",
        "\n",
        "    data = read_data(filename)\n",
        "    print ('length of dataset = ', len(data))\n",
        "    tokenized_data = [word_tokenize(w) for w in [s for s in sent_tokenize(data)]]\n",
        "    vocabulary = set()\n",
        "\n",
        "    for s in tokenized_data:\n",
        "        for w in s:\n",
        "            vocabulary.add(w)\n",
        "\n",
        "    vocabulary = sorted(vocabulary)\n",
        "\n",
        "    for c, i in list(zip(vocabulary, range(len(vocabulary)))):\n",
        "        word_to_lookup[c] = i\n",
        "        lookup_to_word[i] = c\n",
        "\n",
        "    for s in tokenized_data:\n",
        "        for w in s:\n",
        "            encoded_data.append(word_to_lookup[w])\n",
        "\n",
        "    return word_to_lookup, lookup_to_word, encoded_data\n",
        "\n",
        "w2l, l2w, tokenized_data = load_and_encode_data_nltk(\"/content/drive/MyDrive/colab/pg1400.txt\")\n",
        "\n",
        "tensor_tokenized_data = torch.tensor(tokenized_data, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b6a83ba6-a764-470f-8d75-a7c8974c9f5f",
      "metadata": {
        "tags": [],
        "id": "b6a83ba6-a764-470f-8d75-a7c8974c9f5f",
        "outputId": "c46d8211-c463-47d0-e719-6ef6b040705f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([229192]) torch.int64\n",
            "tensor([13697,  1241,   661,  5082,  8781,   647,   537,  1537,  5120,  7431,\n",
            "         5967, 12111, 12807,  8781,  2302,  2307,  7092, 12111,  1607,  1453,\n",
            "         2246,  8407,  8902,  9083,  8781, 12111, 13380,  2509,  8622,  4109,\n",
            "         2246, 13322,  2188,  8622, 10315, 13172,    10,  1779,  8135,  4077,\n",
            "         7440,     8,  6277,  7440,  2584,  8867,  9937,  7440, 12617, 12111,\n",
            "        12069,  8781, 12111,  1241,   661,   904,  7119, 13322, 12169,  5120,\n",
            "         8867,  8837,  2509, 13444,    10,   770, 13477,  2388,  8660,  7883,\n",
            "         7092, 12111,  1607,  1453,     8, 13477, 13265,  6661, 12279,  3453,\n",
            "        12111,  7693,  8781, 12111,  4137, 13181, 13477,  2388,  7883,  2752,\n",
            "        12817, 12169,  5082,    10,  1554,    56,   647,   537,   162,    56])\n"
          ]
        }
      ],
      "source": [
        "print(tensor_tokenized_data.shape, tensor_tokenized_data.dtype)\n",
        "print(tensor_tokenized_data[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a57ea747-587e-4228-8036-333d1625bb30",
      "metadata": {
        "id": "a57ea747-587e-4228-8036-333d1625bb30"
      },
      "source": [
        "Test training dataset split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b85455ca-6b37-4b29-a318-7efbc518c96e",
      "metadata": {
        "tags": [],
        "id": "b85455ca-6b37-4b29-a318-7efbc518c96e"
      },
      "outputs": [],
      "source": [
        "n = int(0.8*len(tensor_tokenized_data))\n",
        "train_data = tensor_tokenized_data[:n]\n",
        "test_data = tensor_tokenized_data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "917c5bb5-004e-4b7e-b0e2-ed13a686dda9",
      "metadata": {
        "tags": [],
        "id": "917c5bb5-004e-4b7e-b0e2-ed13a686dda9",
        "outputId": "61853da0-eb7a-4835-932d-81e58a110c56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([13697,  1241,   661,  ...,     8,  2246, 13181]),\n",
              " tensor([12111, 11712, 12997,  ...,  8589,  5083,    10]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "train_data, test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e1aa9af7-b3e5-4a38-ad56-e0df654ef415",
      "metadata": {
        "id": "e1aa9af7-b3e5-4a38-ad56-e0df654ef415",
        "outputId": "bbd8cc8a-41a3-4e3c-e7c0-d4c38d6b8449",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([13697,  1241,   661,  5082,  8781,   647,   537,  1537,  5120])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "block_size = 8\n",
        "batch_size = 4\n",
        "\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "60b9bc71-ec56-4f12-b607-a1ba866374e8",
      "metadata": {
        "tags": [],
        "id": "60b9bc71-ec56-4f12-b607-a1ba866374e8",
        "outputId": "5d756a88-9bc5-4331-e2c2-eaf44bb76006",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Project Gutenberg eBook of Great Expectations'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "\" \".join([l2w[i] for i in train_data[1:7].numpy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f48937ba-262f-4c2f-aa99-b93c703d93a7",
      "metadata": {
        "id": "f48937ba-262f-4c2f-aa99-b93c703d93a7"
      },
      "source": [
        "## Embedding layers\n",
        "\n",
        "pytorch embedding layers act as a array of trainable parameters for a given vocabulary size. For example, if we have n numbers where we are using the the data in context and target as in the table below, the vocabulary size is n and we can have some embedding paramters depending on the size of the data we are trying to feed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "699930b2-a770-4509-9e1d-e082460ac10c",
      "metadata": {
        "id": "699930b2-a770-4509-9e1d-e082460ac10c"
      },
      "source": [
        "### context size = 1, target size = 1\n",
        "The following example embeds the sequence of numbers up to 10 as the context and target.\n",
        "\n",
        "|context|target|\n",
        "|-|-|\n",
        "|1|2|\n",
        "|2|3|\n",
        "|3|4|\n",
        "|..|..|\n",
        "|n|n+1|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f7a83685-05a5-40ad-9271-8e85dcc547d8",
      "metadata": {
        "tags": [],
        "id": "f7a83685-05a5-40ad-9271-8e85dcc547d8",
        "outputId": "4d85653b-88a8-41e6-841e-d6c9e1257540",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.881331\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.2687, -0.7669, -1.5136,  1.0216, -1.2418, -0.4616, -0.3318,  0.0050,\n",
              "          1.2334,  0.6881]], grad_fn=<IndexBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "vocab_size = 10\n",
        "context_size = 1\n",
        "\n",
        "input_tensor = []\n",
        "for i in range(1,vocab_size-context_size):\n",
        "    input_tensor.append([[i], [i+1]])\n",
        "\n",
        "class BiGram(nn.Module):\n",
        "    def __init__(self, vocab_size, context_size, embedding_dim):\n",
        "        super(BiGram, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size*embedding_dim, 128)\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view(1,-1)\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs\n",
        "\n",
        "\n",
        "model = BiGram(vocab_size,context_size, vocab_size)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "loss_function = nn.NLLLoss()\n",
        "losses = []\n",
        "\n",
        "for epoch in range(100):\n",
        "    # print('epoch: {}'.format(epoch + 1))\n",
        "    running_loss = 0\n",
        "    for data in input_tensor:\n",
        "        word, label = data\n",
        "        word = torch.LongTensor(word)\n",
        "        label = torch.LongTensor(label)\n",
        "        # forward\n",
        "        out = model(word)\n",
        "        loss = F.cross_entropy(out, label)\n",
        "        running_loss += loss.item()\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "print('Loss: {:.6f}'.format(running_loss / vocab_size))\n",
        "\n",
        "model.embeddings.weight[[1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3159220d-e5e9-4f27-86ee-b771b96030d7",
      "metadata": {
        "tags": [],
        "id": "3159220d-e5e9-4f27-86ee-b771b96030d7",
        "outputId": "e603dfdc-8016-4d72-8879-abcc7a6d5b40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "real word is 10, predict word is 5\n"
          ]
        }
      ],
      "source": [
        "pred = randrange(1, 10)\n",
        "target = pred + 1\n",
        "out = model(torch.tensor([pred], dtype=torch.long))\n",
        "_, predict_label = torch.max(out, 1)\n",
        "predict_word = predict_label.data[0].item()\n",
        "print('real word is {}, predict word is {}'.format(target, predict_word))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "667ba96b-592d-44f5-a2be-6fcf1dfca30a",
      "metadata": {
        "tags": [],
        "id": "667ba96b-592d-44f5-a2be-6fcf1dfca30a"
      },
      "source": [
        "### context_size = 2; target_size = 1\n",
        "\n",
        "|context|target|\n",
        "|-|-|\n",
        "|1,2|3|\n",
        "|2,3|4|\n",
        "|4,5|6|\n",
        "|..|..|\n",
        "|n,n+1|n+2|\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "76582750-89cc-46ef-a01d-4206e6b5d10d",
      "metadata": {
        "tags": [],
        "id": "76582750-89cc-46ef-a01d-4206e6b5d10d",
        "outputId": "7b1825be-437d-4c11-be5c-58d6c60bd913",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.823904\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.8044, -2.0680, -0.3244,  0.8827, -0.5343,  1.4524, -0.1523, -1.1447,\n",
              "          1.0708,  0.2457, -1.1162,  1.9035]], grad_fn=<IndexBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "context_size = 2\n",
        "# The end of vocab size might overflow at the boundary like 9+10 = 11. Hence add context size for safely rail.\n",
        "vocab_size = 10+context_size\n",
        "\n",
        "input_tensor = []\n",
        "for i in range(1,vocab_size-context_size):\n",
        "    input_tensor.append([[i, i+1], [i+2]])\n",
        "\n",
        "class BiGram(nn.Module):\n",
        "    def __init__(self, vocab_size, context_size, embedding_dim):\n",
        "        super(BiGram, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size*embedding_dim, 128)\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view(1,-1)\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs\n",
        "\n",
        "model = BiGram(vocab_size,context_size, vocab_size)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "loss_function = nn.NLLLoss()\n",
        "losses = []\n",
        "\n",
        "for epoch in range(100):\n",
        "    # print('epoch: {}'.format(epoch + 1))\n",
        "    running_loss = 0\n",
        "    for data in input_tensor:\n",
        "        word, label = data\n",
        "        word = torch.LongTensor(word)\n",
        "        label = torch.LongTensor(label)\n",
        "        # forward\n",
        "        out = model(word)\n",
        "        loss = F.cross_entropy(out, label)\n",
        "        running_loss += loss.item()\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "print('Loss: {:.6f}'.format(running_loss / vocab_size))\n",
        "\n",
        "model.embeddings.weight[[1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f3149608-901b-4829-996d-89c12879defc",
      "metadata": {
        "tags": [],
        "id": "f3149608-901b-4829-996d-89c12879defc",
        "outputId": "bce8090d-5be7-4128-e038-b306959b1f89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9, 10) 11\n",
            "real word is 11, predict word is 11\n"
          ]
        }
      ],
      "source": [
        "pred = randrange(1, 10)\n",
        "target = pred + 2\n",
        "print ((pred, pred+1), pred+2)\n",
        "out = model(torch.tensor([pred, pred+1], dtype=torch.long))\n",
        "_, predict_label = torch.max(out, 1)\n",
        "predict_word = predict_label.data[0].item()\n",
        "print('real word is {}, predict word is {}'.format(target, predict_word))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5e0f15e-1b4f-41a3-a6e7-2c4370d2d293",
      "metadata": {
        "tags": [],
        "id": "d5e0f15e-1b4f-41a3-a6e7-2c4370d2d293"
      },
      "source": [
        "## String embeddings\n",
        "\n",
        "From our data split above, lets construct the sequences to be read from for testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "1b2df35b-3e51-4a7e-b108-22aefba93fc8",
      "metadata": {
        "tags": [],
        "id": "1b2df35b-3e51-4a7e-b108-22aefba93fc8"
      },
      "outputs": [],
      "source": [
        "def get_data_for_processing(training_data_set):\n",
        "    data = train_data if training_data_set else test_data\n",
        "    sample_indices = torch.randint(len(data)-block_size, (batch_size, ))\n",
        "    x = torch.stack([data[i:i+block_size] for i in sample_indices])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in sample_indices])\n",
        "    return x.to(device), y.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7957b67f-0831-445a-9b8f-8a60fd9f0d30",
      "metadata": {
        "tags": [],
        "id": "7957b67f-0831-445a-9b8f-8a60fd9f0d30",
        "outputId": "027daa2c-36c3-44a3-a61f-bc157add0961",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "do I\n",
            "do I ,\n",
            "do I , ”\n",
            "do I , ” said\n",
            "do I , ” said Mr.\n",
            "do I , ” said Mr. Pumblechook\n",
            "do I , ” said Mr. Pumblechook ,\n",
            "do I , ” said Mr. Pumblechook , getting\n",
            "When she\n",
            "When she left\n",
            "When she left off—and\n",
            "When she left off—and she\n",
            "When she left off—and she had\n",
            "When she left off—and she had not\n",
            "When she left off—and she had not laughed\n",
            "When she left off—and she had not laughed languidly\n",
            "buying such\n",
            "buying such household\n",
            "buying such household stuffs\n",
            "buying such household stuffs and\n",
            "buying such household stuffs and goods\n",
            "buying such household stuffs and goods as\n",
            "buying such household stuffs and goods as required\n",
            "buying such household stuffs and goods as required a\n",
            "who had\n",
            "who had never\n",
            "who had never been\n",
            "who had never been heard\n",
            "who had never been heard of\n",
            "who had never been heard of before\n",
            "who had never been heard of before )\n",
            "who had never been heard of before ) coming\n",
            "13698\n"
          ]
        }
      ],
      "source": [
        "samples_x, samples_y = get_data_for_processing(True)\n",
        "for batch in range(batch_size):\n",
        "    for block in range(block_size):\n",
        "        context = samples_x[batch, :block+1]\n",
        "        target = samples_y[batch, block]\n",
        "        print (\" \".join([l2w[i] for i in context.cpu().numpy()]), l2w[target.item()])\n",
        "\n",
        "vocab_size = len(l2w)\n",
        "print (vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimation_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'test']:\n",
        "    losses = torch.zeros(evaluation_iters)\n",
        "    for k in range(evaluation_iters):\n",
        "      X,Y = get_data_for_processing(True if split == 'train' else False)\n",
        "      logits, loss = model(X,Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n"
      ],
      "metadata": {
        "id": "N64B5mRKCEu_"
      },
      "id": "N64B5mRKCEu_",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "da43ee4c-974b-494c-b83e-5cbf3081eaa2",
      "metadata": {
        "tags": [],
        "id": "da43ee4c-974b-494c-b83e-5cbf3081eaa2",
        "outputId": "593f1df8-a3bc-4d84-ead6-142149c04bf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 13698]) tensor(9.9515, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "class LLM(nn.Module):\n",
        "    def __init__(self, vocab_size, context, embedded_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        logits = self.embedding(idx)\n",
        "\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B,T,C = logits.shape\n",
        "            logits = logits.view(B*T,C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx)\n",
        "            logits = logits[:, -1,  :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "model = LLM(vocab_size, block_size, vocab_size).to(device)\n",
        "logits, loss = model(samples_x, samples_y)\n",
        "print (logits.shape, loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "82867e31-0df6-44c4-a83b-aafe8813fb2d",
      "metadata": {
        "tags": [],
        "id": "82867e31-0df6-44c4-a83b-aafe8813fb2d",
        "outputId": "a0f56c7b-10ae-4286-a8bc-e54ea5836809",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'contemptuously choicer donkey heaven enormous shedding incurred unassertive hauled giants enough. safety tokens Sessions tools confront cleared. empty-handed snake eyes—a —Then hurt. hurried glass credence Skiffins dexterity conviction blackest-looking shorter contradicted mo— consorted surveyor hesitated pencil apologetically scandalised injudicious preceded one-eyed'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "print(device)\n",
        "g = model.generate(idx=torch.tensor([[4002]], dtype=torch.long).to(device), max_new_tokens=40)\n",
        "\" \".join([l2w[i] for i in g.cpu().numpy()[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "0b544b4e-c706-4cd2-8916-bf07c55300a0",
      "metadata": {
        "id": "0b544b4e-c706-4cd2-8916-bf07c55300a0"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "evaluation_iters = 200\n",
        "for step in range(1000):\n",
        "    if step % evaluation_iters == 0:\n",
        "      losses = estimation_loss()\n",
        "      print (losses)\n",
        "      print(\"Training data loss = \", losses['train'], \"Test data loss = \", losses['test'])\n",
        "    samples_x, samples_y = get_data_for_processing(True)\n",
        "    logits, loss = model(samples_x, samples_y)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if step % 100 == 0:\n",
        "        print (loss.item())\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-tjz64nAEA4",
        "outputId": "699d3352-08e0-4e6d-c2d7-c71bd1642e2d"
      },
      "id": "4-tjz64nAEA4",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': tensor(9.9856), 'test': tensor(9.9900)}\n",
            "Training data loss =  tensor(9.9856) Test data loss =  tensor(9.9900)\n",
            "10.066347122192383\n",
            "9.968426704406738\n",
            "{'train': tensor(9.8687), 'test': tensor(9.8892)}\n",
            "Training data loss =  tensor(9.8687) Test data loss =  tensor(9.8892)\n",
            "9.848640441894531\n",
            "9.766807556152344\n",
            "{'train': tensor(9.7412), 'test': tensor(9.7805)}\n",
            "Training data loss =  tensor(9.7412) Test data loss =  tensor(9.7805)\n",
            "9.731710433959961\n",
            "9.754940032958984\n",
            "{'train': tensor(9.5936), 'test': tensor(9.6585)}\n",
            "Training data loss =  tensor(9.5936) Test data loss =  tensor(9.6585)\n",
            "9.703038215637207\n",
            "9.542753219604492\n",
            "{'train': tensor(9.4666), 'test': tensor(9.5497)}\n",
            "Training data loss =  tensor(9.4666) Test data loss =  tensor(9.5497)\n",
            "9.387492179870605\n",
            "9.384162902832031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g = model.generate(idx=torch.tensor([[3000]], dtype=torch.long).to(device), max_new_tokens=40)\n",
        "\" \".join([l2w[i] for i in g.cpu().numpy()[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "xJxKeP9TAF9P",
        "outputId": "1c654cba-279d-40f3-b295-a53fd6c7c800"
      },
      "id": "xJxKeP9TAF9P",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'books spring weight boarder common performing scores eighteen-pence expense— joviality depreciation Tramping material shriek sorrowful gird retiring merits THIS Naturally kissing 3 effectually home-voice hailing blubbered sluice charmed threshold tall On-common seventh unfortunately available lounging honoured : saddler respect. awaits carriages'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lAxUsTHHAJO_"
      },
      "id": "lAxUsTHHAJO_",
      "execution_count": 21,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}