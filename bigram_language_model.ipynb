{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f48937ba-262f-4c2f-aa99-b93c703d93a7",
      "metadata": {
        "id": "f48937ba-262f-4c2f-aa99-b93c703d93a7"
      },
      "source": [
        "## Bigram \n",
        "This file demonstrates a bigram model.\n",
        "\n",
        "### Embedding layers\n",
        "\n",
        "pytorch embedding layers act as a array of trainable parameters for a given vocabulary size. For example, if we have n numbers where we are using the the data in context and target as in the table below, the vocabulary size is n and we can have some embedding paramters depending on the size of the data we are trying to feed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "699930b2-a770-4509-9e1d-e082460ac10c",
      "metadata": {
        "id": "699930b2-a770-4509-9e1d-e082460ac10c"
      },
      "source": [
        "### context size = 1, target size = 1\n",
        "The following example embeds the sequence of numbers up to 10 as the context and target.\n",
        "\n",
        "|context|target|\n",
        "|-|-|\n",
        "|1|2|\n",
        "|2|3|\n",
        "|3|4|\n",
        "|..|..|\n",
        "|n|n+1|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f7a83685-05a5-40ad-9271-8e85dcc547d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7a83685-05a5-40ad-9271-8e85dcc547d8",
        "outputId": "4d85653b-88a8-41e6-841e-d6c9e1257540",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.881331\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[-0.2687, -0.7669, -1.5136,  1.0216, -1.2418, -0.4616, -0.3318,  0.0050,\n",
              "          1.2334,  0.6881]], grad_fn=<IndexBackward0>)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "vocab_size = 10\n",
        "context_size = 1\n",
        "\n",
        "input_tensor = []\n",
        "for i in range(1,vocab_size-context_size):\n",
        "    input_tensor.append([[i], [i+1]])\n",
        "\n",
        "class BiGram(nn.Module):\n",
        "    def __init__(self, vocab_size, context_size, embedding_dim):\n",
        "        super(BiGram, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size*embedding_dim, 128)\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view(1,-1)\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs\n",
        "\n",
        "\n",
        "model = BiGram(vocab_size,context_size, vocab_size)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "loss_function = nn.NLLLoss()\n",
        "losses = []\n",
        "\n",
        "for epoch in range(100):\n",
        "    # print('epoch: {}'.format(epoch + 1))\n",
        "    running_loss = 0\n",
        "    for data in input_tensor:\n",
        "        word, label = data\n",
        "        word = torch.LongTensor(word)\n",
        "        label = torch.LongTensor(label)\n",
        "        # forward\n",
        "        out = model(word)\n",
        "        loss = F.cross_entropy(out, label)\n",
        "        running_loss += loss.item()\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "print('Loss: {:.6f}'.format(running_loss / vocab_size))\n",
        "\n",
        "model.embeddings.weight[[1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3159220d-e5e9-4f27-86ee-b771b96030d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3159220d-e5e9-4f27-86ee-b771b96030d7",
        "outputId": "e603dfdc-8016-4d72-8879-abcc7a6d5b40",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "real word is 10, predict word is 5\n"
          ]
        }
      ],
      "source": [
        "pred = randrange(1, 10)\n",
        "target = pred + 1\n",
        "out = model(torch.tensor([pred], dtype=torch.long))\n",
        "_, predict_label = torch.max(out, 1)\n",
        "predict_word = predict_label.data[0].item()\n",
        "print('real word is {}, predict word is {}'.format(target, predict_word))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "667ba96b-592d-44f5-a2be-6fcf1dfca30a",
      "metadata": {
        "id": "667ba96b-592d-44f5-a2be-6fcf1dfca30a",
        "tags": []
      },
      "source": [
        "### context_size = 2; target_size = 1\n",
        "\n",
        "|context|target|\n",
        "|-|-|\n",
        "|1,2|3|\n",
        "|2,3|4|\n",
        "|4,5|6|\n",
        "|..|..|\n",
        "|n,n+1|n+2|\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "76582750-89cc-46ef-a01d-4206e6b5d10d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76582750-89cc-46ef-a01d-4206e6b5d10d",
        "outputId": "7b1825be-437d-4c11-be5c-58d6c60bd913",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.823904\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[-0.8044, -2.0680, -0.3244,  0.8827, -0.5343,  1.4524, -0.1523, -1.1447,\n",
              "          1.0708,  0.2457, -1.1162,  1.9035]], grad_fn=<IndexBackward0>)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "context_size = 2\n",
        "# The end of vocab size might overflow at the boundary like 9+10 = 11. Hence add context size for safely rail.\n",
        "vocab_size = 10+context_size\n",
        "\n",
        "input_tensor = []\n",
        "for i in range(1,vocab_size-context_size):\n",
        "    input_tensor.append([[i, i+1], [i+2]])\n",
        "\n",
        "class BiGram(nn.Module):\n",
        "    def __init__(self, vocab_size, context_size, embedding_dim):\n",
        "        super(BiGram, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size*embedding_dim, 128)\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view(1,-1)\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs\n",
        "\n",
        "model = BiGram(vocab_size,context_size, vocab_size)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "loss_function = nn.NLLLoss()\n",
        "losses = []\n",
        "\n",
        "for epoch in range(100):\n",
        "    # print('epoch: {}'.format(epoch + 1))\n",
        "    running_loss = 0\n",
        "    for data in input_tensor:\n",
        "        word, label = data\n",
        "        word = torch.LongTensor(word)\n",
        "        label = torch.LongTensor(label)\n",
        "        # forward\n",
        "        out = model(word)\n",
        "        loss = F.cross_entropy(out, label)\n",
        "        running_loss += loss.item()\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "print('Loss: {:.6f}'.format(running_loss / vocab_size))\n",
        "\n",
        "model.embeddings.weight[[1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f3149608-901b-4829-996d-89c12879defc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3149608-901b-4829-996d-89c12879defc",
        "outputId": "bce8090d-5be7-4128-e038-b306959b1f89",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(9, 10) 11\n",
            "real word is 11, predict word is 11\n"
          ]
        }
      ],
      "source": [
        "pred = randrange(1, 10)\n",
        "target = pred + 2\n",
        "print ((pred, pred+1), pred+2)\n",
        "out = model(torch.tensor([pred, pred+1], dtype=torch.long))\n",
        "_, predict_label = torch.max(out, 1)\n",
        "predict_word = predict_label.data[0].item()\n",
        "print('real word is {}, predict word is {}'.format(target, predict_word))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
